{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a0dc67b-350b-4552-aaa8-850beb3b4374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training\n",
      "Epoch [1/10], Loss: 0.9377\n",
      "Epoch [2/10], Loss: 0.9254\n",
      "Epoch [3/10], Loss: 0.9254\n",
      "Epoch [4/10], Loss: 0.9254\n",
      "Epoch [5/10], Loss: 0.9254\n",
      "Epoch [6/10], Loss: 0.9254\n",
      "Epoch [7/10], Loss: 0.9254\n",
      "Epoch [8/10], Loss: 0.9254\n",
      "Epoch [9/10], Loss: 0.9254\n",
      "Epoch [10/10], Loss: 0.9254\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "# Define the decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28*28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        return x\n",
    "\n",
    "# Combine encoder and decoder to create autoencoder\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "device = torch.device(\"cuda\")\n",
    "# Initialize the autoencoder\n",
    "autoencoder = AutoEncoder().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "print(\"Started Training\")\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        inputs, _ = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(inputs.to(device))\n",
    "        loss = criterion(outputs, inputs.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Save the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1282abd-7aac-4f93-a672-59947edcfb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 12259.0868\n",
      "Epoch [2/10], Loss: 8893.5192\n",
      "Epoch [3/10], Loss: 7773.3711\n",
      "Epoch [4/10], Loss: 7266.3588\n",
      "Epoch [5/10], Loss: 7015.4940\n",
      "Epoch [6/10], Loss: 6766.0795\n",
      "Epoch [7/10], Loss: 6525.6534\n",
      "Epoch [8/10], Loss: 6372.7353\n",
      "Epoch [9/10], Loss: 6250.8713\n",
      "Epoch [10/10], Loss: 6123.8647\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "# Define the decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(10, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28*28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        return x\n",
    "\n",
    "# Combine encoder and decoder to create autoencoder\n",
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VariationalAutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.mean = nn.Linear(32,10)\n",
    "        self.std = nn.Linear(32,10)\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean = self.mean(x)\n",
    "        std = self.std(x)\n",
    "        z = mean + std * torch.randn_like(std)\n",
    "        x = self.decoder(z)\n",
    "        return x, mean, std\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    # transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "# Initialize the autoencoder\n",
    "autoencoder = VariationalAutoEncoder()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "def criterion(pred,y,mean, logvar):\n",
    "    re = nn.functional.binary_cross_entropy(pred, y, reduction = 'sum')\n",
    "    kld = -0.5*torch.sum(1+logvar - mean.pow(2) - logvar.exp())\n",
    "    return re + kld\n",
    "optim = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        inputs, _ = data\n",
    "        inputs = inputs\n",
    "        optim.zero_grad()\n",
    "        outputs, mean, std = autoencoder(inputs.view(64,-1))\n",
    "        loss = criterion(outputs, inputs, mean, std)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b27f0a19-d55d-4999-b762-5298f21b65b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def generate():\n",
    "    mean = torch.zeros((1,10))\n",
    "    var = torch.ones((1,10))\n",
    "    z = mean + var * torch.randn_like(var)\n",
    "    xDecoded = autoencoder.decoder(z)\n",
    "    digit = xDecoded.detach().reshape(28,28)\n",
    "    plt.imshow(digit, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "generate()\n",
    "generate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82be7923-21ee-4a5c-a7ef-ab91f99be43e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
